{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype/SNP relation extraction from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "\n",
    "# import snorkel and gwasdb\n",
    "sys.path.append('../snorkel')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/crawler')\n",
    "\n",
    "# set up paths\n",
    "abstract_dir = '../data/db/papers'\n",
    "\n",
    "# set up matplotlib\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import XMLDocParser\n",
    "from extractor.parser import UnicodeXMLTableDocParser\n",
    "\n",
    "xml_parser = UnicodeXMLTableDocParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    text='.//table',\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 33s, sys: 48.9 s, total: 16min 22s\n",
      "Wall time: 23min 42s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import HTMLParser\n",
    "from extractor.parser import UnicodeTableParser\n",
    "from snorkel.parser import CorpusParser\n",
    "import cPickle\n",
    "\n",
    "table_parser = UnicodeTableParser()\n",
    "html_parser = HTMLParser(path='../data/db/papers/')\n",
    "\n",
    "corpus_name = 'gwas-table-corpus.pkl'\n",
    "\n",
    "try:\n",
    "    with open(corpus_name,\"r\") as pkl:\n",
    "        corpus = cPickle.load(pkl)\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, table_parser)\n",
    "    %time corpus = cp.parse_corpus(name='GWAS Corpus')\n",
    "    # pickling currently doesn't work...\n",
    "#     with open(corpus_name,\"w\") as pkl:\n",
    "#         corpus = cPickle.dump(corpus, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus15 = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSid Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, RegexMatchSpan, Union\n",
    "from snorkel.candidates import EntityExtractor\n",
    "from snorkel.candidates import TableNgrams\n",
    "\n",
    "from db.kb import KnowledgeBase\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = TableNgrams(n_max=1)\n",
    "\n",
    "# Get a list of all the RSids we know\n",
    "kb = KnowledgeBase()\n",
    "rs_ids = kb.get_rsid_candidates()\n",
    "\n",
    "# Define matchers\n",
    "dict_rsid_matcher = DictionaryMatch(d=rs_ids, longest_match_only=False)\n",
    "regx_rsid_matcher = RegexMatchSpan(rgx=r'rs\\d+')\n",
    "rsid_matcher = Union(dict_rsid_matcher, regx_rsid_matcher)\n",
    "\n",
    "rsid_extractor = EntityExtractor(ngrams, rsid_matcher)\n",
    "# %time rs_candidates = rsid_extractor.extract(corpus.get_tables(), name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for cand in rs_candidates[:10]: \n",
    "#     print cand\n",
    "# print \"%s candidates extracted\" % len(rs_candidates)\n",
    "# print rs_candidates[0].context\n",
    "# print rs_candidates[0].context.cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rs_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-3ee852b6b2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgold_set_rsids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrs_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgold_rsid_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rs_candidates' is not defined"
     ]
    }
   ],
   "source": [
    "from extractor.util import gold_rsid_stats, gold_rsid_precision\n",
    "\n",
    "gold_set = frozenset( [ (doc.name, rs_id) for doc in corpus.documents for rs_id in kb.rsids_by_pmid(int(doc.name)) ] )\n",
    "gold_set_rsids = [rs_id for doc_id, rs_id in gold_set]\n",
    "\n",
    "gold_rsid_stats(rs_candidates, gold_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: some SNPs seem to be never mentioned (e.g. rs12122100) while others (rs727153) appear only in the text.\n",
    "Sometimes, it's not picked up for a different, strange reason: see rs13314993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cells = rs_candidates[0].context.cell.aligned_cells('row')\n",
    "[cell.text for cell in cells]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, Union, CellDictNameMatcher\n",
    "from snorkel.candidates import EntityExtractor\n",
    "from snorkel.candidates import TableNgrams, CellSpace\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = TableNgrams(n_max=9)\n",
    "cells = CellSpace()\n",
    "\n",
    "# Create a list of possible words that could denote phenotypes\n",
    "phen_words = ['trait', 'phenotype']\n",
    "\n",
    "# Define matchers\n",
    "# dict_row_matcher = DictionaryMatch(d=phen_words, longest_match_only=False, stemmer='porter')\n",
    "# cell_row_matcher = CellNameMatcher(row_matcher=dict_row_matcher, cand_space=ngrams)\n",
    "# dict_col_matcher = DictionaryMatch(d=phen_words, longest_match_only=False, stemmer='porter')\n",
    "# cell_col_matcher = CellNameMatcher(col_matcher=dict_col_matcher, cand_space=ngrams)\n",
    "# phen_matcher = Union(cell_row_matcher, cell_col_matcher)\n",
    "# phen_matcher = CellNameMatcher(col_matcher=dict_col_matcher, cand_space=ngrams)\n",
    "phen_matcher = CellDictNameMatcher(axis='col', d=phen_words, n_max=3, ignore_case=True)\n",
    "\n",
    "phen_extractor = EntityExtractor(cells, phen_matcher)\n",
    "# %time phen_candidates = phen_extractor.extract(corpus.get_tables(), name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from db.kb import KnowledgeBase\n",
    "from snorkel.utils import slice_into_ngrams\n",
    "from snorkel.matchers import FullCellDictMatcher\n",
    "\n",
    "def make_ngrams(L, n_max=10, n_min=3, delim=' '):\n",
    "    for l in L:\n",
    "        yield l\n",
    "        tokens = l.strip().split(delim)\n",
    "        for ngram in slice_into_ngrams(tokens, n_max=n_max, n_min=n_min, delim=delim):\n",
    "            yield ngram\n",
    "\n",
    "# collect phenotype list\n",
    "kb = KnowledgeBase()\n",
    "# efo phenotypes\n",
    "efo_phenotype_list0 = kb.get_phenotype_candidates(source='efo', peek=True) # TODO: remove peaking\n",
    "efo_phenotype_list = list(make_ngrams(efo_phenotype_list0))\n",
    "# mesh diseases\n",
    "mesh_phenotype_list0 = kb.get_phenotype_candidates(source='mesh')\n",
    "mesh_phenotype_list = list(make_ngrams(mesh_phenotype_list0))\n",
    "# mesh chemicals\n",
    "chem_phenotype_list = kb.get_phenotype_candidates(source='chemical')\n",
    "\n",
    "phenotype_names = efo_phenotype_list + mesh_phenotype_list + chem_phenotype_list\n",
    "full_cell_matcher = FullCellDictMatcher(d=phenotype_names, ignore_case=True, stemmer='porter')\n",
    "\n",
    "cells = CellSpace()\n",
    "phen_extractor2 = EntityExtractor(cells, full_cell_matcher)\n",
    "select_tables = [table for table in corpus.get_tables() if table.document.name == '19197348']\n",
    "# %time phen_candidates = phen_extractor2.extract(select_tables, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(\"BMI\", context=None, chars=[0,2], words=[0,0])\n",
      "Span(\"Height\", context=None, chars=[0,5], words=[0,0])\n",
      "Span(\"Waist Circumference\", context=None, chars=[0,18], words=[0,1])\n",
      "Span(\"Weight\", context=None, chars=[0,5], words=[0,0])\n",
      "Span(\"Fasting plasma glucose\", context=None, chars=[0,21], words=[0,2])\n",
      "Span(\"Thyroid Stimulating Hormone\", context=None, chars=[0,26], words=[0,2])\n",
      "Span(\"BMI\", context=None, chars=[0,2], words=[0,0])\n",
      "Span(\"Height\", context=None, chars=[0,5], words=[0,0])\n",
      "Span(\"Fasting plasma glucose\", context=None, chars=[0,21], words=[0,2])\n",
      "Span(\"Thyroid Stimulating Hormone\", context=None, chars=[0,26], words=[0,2])\n"
     ]
    }
   ],
   "source": [
    "for c in (phen_candidates):\n",
    "    print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 candidates extracted\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 11, u'BMI')\n",
      "Span(\"BMI\", context=None, chars=[0,2], words=[0,0])\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 23, u'Height')\n",
      "Span(\"Height\", context=None, chars=[0,5], words=[0,0])\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 46, u'Waist Circumference')\n",
      "Span(\"Waist Circumference\", context=None, chars=[0,18], words=[0,1])\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 102, u'Weight')\n",
      "Span(\"Weight\", context=None, chars=[0,5], words=[0,0])\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 540, u'Fasting plasma glucose')\n",
      "Span(\"Fasting plasma glucose\", context=None, chars=[0,21], words=[0,2])\n",
      "19197348 Table('19197348', 2) Cell('19197348', 2, 552, u'Thyroid Stimulating Hormone')\n",
      "Span(\"Thyroid Stimulating Hormone\", context=None, chars=[0,26], words=[0,2])\n",
      "19197348 Table('19197348', 3) Cell('19197348', 3, 22, u'BMI')\n",
      "Span(\"BMI\", context=None, chars=[0,2], words=[0,0])\n",
      "19197348 Table('19197348', 3) Cell('19197348', 3, 49, u'Height')\n",
      "Span(\"Height\", context=None, chars=[0,5], words=[0,0])\n",
      "19197348 Table('19197348', 3) Cell('19197348', 3, 508, u'Fasting plasma glucose')\n",
      "Span(\"Fasting plasma glucose\", context=None, chars=[0,21], words=[0,2])\n",
      "19197348 Table('19197348', 3) Cell('19197348', 3, 535, u'Thyroid Stimulating Hormone')\n",
      "Span(\"Thyroid Stimulating Hormone\", context=None, chars=[0,26], words=[0,2])\n",
      "\n",
      "Phrase('19197348', 2, 11, 0, u'BMI')\n",
      "19197348 Table('19197348', 2)\n",
      "Cell('19197348', 2, 11, u'BMI')\n"
     ]
    }
   ],
   "source": [
    "print \"%s candidates extracted\" % len(phen_candidates)\n",
    "for cand in phen_candidates[0:10]: \n",
    "    print cand.context.document.name, cand.context.table, cand.context.cell\n",
    "    print unicode(cand)\n",
    "#     print [span for span in cand.row_ngrams()]\n",
    "#     print [span for span in cand.col_ngrams()]\n",
    "#     print\n",
    "print\n",
    "print phen_candidates[0].context\n",
    "print phen_candidates[0].context.document.name, phen_candidates[0].context.table\n",
    "print phen_candidates[0].context.cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import AlignedTableRelationExtractor, SpanningTableRelationExtractor\n",
    "relation_extractor = AlignedTableRelationExtractor(rsid_extractor, phen_extractor, axis='row', induced=True)\n",
    "relation_extractor2 = SpanningTableRelationExtractor(rsid_extractor, phen_extractor2, axis='row')\n",
    "\n",
    "\n",
    "tables = corpus.get_tables()\n",
    "\n",
    "# create smaller subsets for evaluation/debugging\n",
    "easy_tables = [tables[8]]\n",
    "# hard_tables = [t for t in tables if t.document.name=='17658951']\n",
    "hard_doc = [d for d in corpus.documents if d.name == '17903293'][0]\n",
    "hard_tables = [hard_doc.tables[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 1 0\n",
      "2 3 3 0\n",
      "4 3 1 0\n",
      "4 3 3 0\n",
      "5 3 1 0\n",
      "5 3 3 0\n",
      "CPU times: user 10.6 s, sys: 405 ms, total: 11 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%time candidates = relation_extractor2.extract([select_tables[2]], name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cand in candidates[:10]: \n",
    "    print cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 46min 52s, sys: 11min 26s, total: 2h 58min 18s\n",
      "Wall time: 3h 36min 15s\n",
      "3561 relations extracted, e.g.\n",
      "SpanPair(Span(\"rs1158167\", context=None, chars=[0,8], words=[0,0]), Span(\"CysC\", context=None, chars=[0,3], words=[0,0]))\n",
      "SpanPair(Span(\"rs1712790\", context=None, chars=[0,8], words=[0,0]), Span(\"UAE\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs6977660\", context=None, chars=[0,8], words=[0,0]), Span(\"TSH\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs9322817\", context=None, chars=[0,8], words=[0,0]), Span(\"TSH\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs10499559\", context=None, chars=[0,9], words=[0,0]), Span(\"TSH\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs9305354\", context=None, chars=[0,8], words=[0,0]), Span(\"UAE\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs2145231\", context=None, chars=[0,8], words=[0,0]), Span(\"CysC\", context=None, chars=[0,3], words=[0,0]))\n",
      "SpanPair(Span(\"rs723464\", context=None, chars=[0,7], words=[0,0]), Span(\"UAE\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs2113379\", context=None, chars=[0,8], words=[0,0]), Span(\"UAE\", context=None, chars=[0,2], words=[0,0]))\n",
      "SpanPair(Span(\"rs2839235\", context=None, chars=[0,8], words=[0,0]), Span(\"GFR\", context=None, chars=[0,2], words=[0,0]))\n"
     ]
    }
   ],
   "source": [
    "%time candidates = relation_extractor.extract(tables, name='all')\n",
    "print \"%s relations extracted, e.g.\" % len(candidates)\n",
    "for cand in candidates[:10]: \n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we remove nested candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 candidates dropped, now we have 1835\n"
     ]
    }
   ],
   "source": [
    "# load existing candidates into a dict\n",
    "span_dict = { str(span_pair.span1.context) : list() for span_pair in candidates }\n",
    "for span_pair in candidates:\n",
    "    span = span_pair.span1\n",
    "    span_dict[str(span.context)].append( (span.char_start, span.char_end) )\n",
    "\n",
    "def nested(ivl1, ivl2):\n",
    "    if ivl1 != ivl2 and ivl2[0] <= ivl1[0] <= ivl1[1] <= ivl2[1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "new_candidates = list()\n",
    "for span_pair in candidates:\n",
    "    span = span_pair.span1\n",
    "    span_ivl = span.char_start, span.char_end\n",
    "    span_name = str(span.context)\n",
    "    if all([not nested(span_ivl, other_ivl) for other_ivl in span_dict[span_name]]):\n",
    "        new_candidates.append(span_pair)\n",
    "        \n",
    "print len(candidates) - len(new_candidates), 'candidates dropped, now we have', len(new_candidates)\n",
    "# phen_c = new_phen_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# candidates15 = candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a gold set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a gold set, we save all extracted relations into a csv file. We annotate it manually, and save the result to a second file. It contains pairs of phenotype and rsid strings; if that file exists, we take these as gold truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store relations to annotate\n",
    "with open('rels.acroynms.unnanotated.tsv', 'w') as f:\n",
    "    for span_pair in new_candidates:\n",
    "        doc_id = span_pair.span0.context.document.name\n",
    "        table_id = span_pair.span0.context.table.position\n",
    "        row_num = span_pair.span0.context.cell.row_num\n",
    "        str1 = span_pair.span0.get_span()\n",
    "        str2 = span_pair.span1.get_span()\n",
    "        try:\n",
    "            f.write('%s\\t%s\\t%d\\t%s\\t%s\\n' % (doc_id, table_id, row_num, str1, str2))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load annotations\n",
    "annotations = dict()\n",
    "with open('rels.acronyms.annotated.txt') as f:\n",
    "    text = f.read()\n",
    "    for line in text.split('\\r'):\n",
    "        doc_id, table_id, col_n, rs_id, phen, res = line.strip().split('\\t')\n",
    "        res = 1 if int(res) == 1 else -1\n",
    "        annotations[(doc_id, table_id, rs_id, phen)] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classify correct relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature index...\n",
      "Extracting features...\n",
      "0/29079\n",
      "5000/29079\n",
      "10000/29079\n",
      "15000/29079\n",
      "20000/29079\n",
      "25000/29079\n"
     ]
    }
   ],
   "source": [
    "from snorkel.features import TableNgramPairFeaturizer\n",
    "\n",
    "pkl_f = 'acro_table_feats.pkl'\n",
    "try:\n",
    "    with open(pkl_f, 'rb') as f:\n",
    "        featurizer = cPickle.load(f)\n",
    "except:\n",
    "    featurizer = TableNgramPairFeaturizer()\n",
    "    featurizer.fit_transform(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1013\n",
      "Gold set size: 822\n",
      "Positive labels in training set: 661\n",
      "Negative labels in training set: 0\n",
      "Positive labels in gold set: 683\n",
      "Negative labels in gold set: 139\n"
     ]
    }
   ],
   "source": [
    "def spair2uid(span_pair):\n",
    "    doc_id = span_pair.span0.context.document.name\n",
    "    table_id = str(span_pair.span0.context.table.position)\n",
    "    str1 = span_pair.span0.get_span()\n",
    "    str2 = span_pair.span1.get_span()\n",
    "    return (doc_id, table_id, str1, str2)\n",
    "\n",
    "# Split into train and test set\n",
    "training_candidates = []\n",
    "gold_candidates     = []\n",
    "gold_labels         = []\n",
    "n_half = len(candidates)/2\n",
    "for c in candidates[:n_half]:\n",
    "    uid = spair2uid(c)\n",
    "    if uid in annotations:\n",
    "        gold_candidates.append(c)\n",
    "        gold_labels.append(annotations[uid])\n",
    "    else:\n",
    "        training_candidates.append(c)\n",
    "training_candidates.extend(candidates[n_half:])\n",
    "gold_labels = np.array(gold_labels)\n",
    "print \"Training set size: %s\" % len(training_candidates)\n",
    "print \"Gold set size: %s\" % len(gold_candidates)\n",
    "print \"Positive labels in training set: %s\" % len([c for c in training_candidates if annotations.get(spair2uid(c),0)==1])\n",
    "print \"Negative labels in training set: %s\" % len([c for c in training_candidates if annotations.get(spair2uid(c),0)==-1])\n",
    "print \"Positive labels in gold set: %s\" % len([c for c in gold_candidates if annotations[spair2uid(c)]==1])\n",
    "print \"Negative labels in gold set: %s\" % len([c for c in gold_candidates if annotations[spair2uid(c)]==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_words = ['rs number', 'rs id', 'rsid']\n",
    "\n",
    "# negative LFs\n",
    "def LF_number(m):\n",
    "    txt = m.span1.get_span()\n",
    "    frac_num = len([ch for ch in txt if ch.isdigit()]) / float(len(txt))\n",
    "    return -1 if len(txt) > 5 and frac_num > 0.4 or frac_num > 0.6 else 0\n",
    "\n",
    "def LF_bad_phen_mentions(m):\n",
    "    top_cells = m.span1.context.cell.aligned_cells(axis='col', induced=True)\n",
    "    top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "    if not top_phrases: return 0\n",
    "    matching_phrases = []\n",
    "    for phrase in top_phrases:\n",
    "        if any (phen_matcher._f_span(word) for word in phrase.text.split(' ')):\n",
    "            matching_phrases.append(phrase)\n",
    "    small_matching_phrases = [phrase for phrase in matching_phrases if len(phrase.text) <= 25]\n",
    "    return -1 if not small_matching_phrases else 0\n",
    "\n",
    "def LF_bad_word(m):\n",
    "    txt = m.span1.get_span()\n",
    "    return -1 if any(word in txt for word in bad_words) else 0\n",
    "\n",
    "LF_tables_neg = [LF_number, LF_bad_phen_mentions]\n",
    "\n",
    "# positive LFs\n",
    "def LF_no_neg(m):\n",
    "    return +1 if not any(LF(m) for LF in LF_tables_neg) else 0\n",
    "\n",
    "LF_tables_pos = [LF_no_neg]\n",
    "\n",
    "LF_tables = LF_tables_neg + LF_tables_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/__init__.py:1155: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LFs...\n",
      "Featurizing...\n",
      "Building feature index...\n",
      "Extracting features...\n",
      "0/19661\n",
      "5000/19661\n",
      "10000/19661\n",
      "15000/19661\n",
      "============================================================\n",
      "LF Summary Statistics: 3 LFs applied to 1013 candidates\n",
      "------------------------------------------------------------\n",
      "Coverage (candidates w/ > 0 labels):\t\t100.00%\n",
      "Overlap (candidates w/ > 1 labels):\t\t0.00%\n",
      "Conflict (candidates w/ conflicting labels):\t0.00%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from snorkel.snorkel import TrainingSet\n",
    "from snorkel.features import NgramFeaturizer\n",
    "\n",
    "training_set = TrainingSet(training_candidates, LF_tables, featurizer=TableNgramPairFeaturizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing mu = 1.00e-05, lf_w0 = 1.00e+00\n",
      "============================================================\n",
      "Begin training for rate=0.01, mu=1e-05\n",
      "\tLearning epoch = 0\tGradient mag. = 0.027386\n",
      "\tLearning epoch = 250\tGradient mag. = 0.046833\n",
      "\tLearning epoch = 500\tGradient mag. = 0.074606\n",
      "\tLearning epoch = 750\tGradient mag. = 0.114826\n",
      "Final gradient magnitude for rate=0.01, mu=1e-05: 0.172\n",
      "Applying LFs...\n",
      "Featurizing...\n",
      "============================================================\n",
      "Testing mu = 1.00e-05, lf_w0 = 2.00e+00\n",
      "============================================================\n",
      "Begin training for rate=0.01, mu=1e-05\n",
      "\tLearning epoch = 0\tGradient mag. = 0.053098\n",
      "\tLearning epoch = 250\tGradient mag. = 0.084777\n",
      "\tLearning epoch = 500\tGradient mag. = 0.121184\n",
      "\tLearning epoch = 750\tGradient mag. = 0.168109\n",
      "Final gradient magnitude for rate=0.01, mu=1e-05: 0.220\n",
      "============================================================\n",
      "Testing mu = 1.00e-07, lf_w0 = 1.00e+00\n",
      "============================================================\n",
      "Begin training for rate=0.01, mu=1e-07\n",
      "\tLearning epoch = 0\tGradient mag. = 0.027386\n",
      "\tLearning epoch = 250\tGradient mag. = 0.046932\n",
      "\tLearning epoch = 500\tGradient mag. = 0.074838\n",
      "\tLearning epoch = 750\tGradient mag. = 0.115220\n",
      "Final gradient magnitude for rate=0.01, mu=1e-07: 0.172\n",
      "============================================================\n",
      "Testing mu = 1.00e-07, lf_w0 = 2.00e+00\n",
      "============================================================\n",
      "Begin training for rate=0.01, mu=1e-07\n",
      "\tLearning epoch = 0\tGradient mag. = 0.053098\n",
      "\tLearning epoch = 250\tGradient mag. = 0.084851\n",
      "\tLearning epoch = 500\tGradient mag. = 0.121324\n",
      "\tLearning epoch = 750\tGradient mag. = 0.168308\n",
      "Final gradient magnitude for rate=0.01, mu=1e-07: 0.221\n"
     ]
    }
   ],
   "source": [
    "from snorkel.snorkel import Learner\n",
    "import snorkel.learning\n",
    "from snorkel.learning import LogReg\n",
    "\n",
    "learner = Learner(training_set, model=LogReg())\n",
    "\n",
    "# Splitting into CV and test set\n",
    "n_half = len(gold_candidates)/2\n",
    "test_candidates = gold_candidates[:n_half]\n",
    "test_labels     = gold_labels[:n_half]\n",
    "cv_candidates   = gold_candidates[n_half:]\n",
    "cv_labels       = gold_labels[n_half:]\n",
    "\n",
    "from snorkel.learning_utils import GridSearch\n",
    "gs       = GridSearch(learner, ['mu', 'lf_w0'], [[1e-5, 1e-7],[1.0,2.0]])\n",
    "gs_stats = gs.fit(cv_candidates, cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LFs...\n",
      "Featurizing...\n",
      "========================================\n",
      "Test set size:\t411\n",
      "----------------------------------------\n",
      "Precision:\t1.0\n",
      "Recall:\t\t1.0\n",
      "F1 Score:\t1.0\n",
      "----------------------------------------\n",
      "TP: 310 | FP: 0 | TN: 101 | FN: 0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "learner.test_wmv(test_candidates, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a695cf16e258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preds = learner.predict_wmv(candidates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macronyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspair2uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmislabeled_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspair2uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspair2uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmislabeled_cand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# preds = learner.predict_wmv(candidates)\n",
    "acronyms = [spair2uid(c) for (c, p) in zip(candidates, preds) if p == 1]\n",
    "mislabeled_cand = [(c,p, annotations.get(spair2uid(c), None)) for c, p in zip(candidates, preds) if p != annotations.get(spair2uid(c), p)]\n",
    "for (c,p,g) in mislabeled_cand[:20]:\n",
    "    print c.span0.context.document.name, p, g\n",
    "    print c.span0.context    \n",
    "    print c.span0.get_span(), c.span1.get_span()\n",
    "    txt = c.span1.get_span()\n",
    "    top_cells = c.span1.context.cell.aligned_cells(axis='col', induced=True)\n",
    "    top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "    print top_phrases\n",
    "    matching_phrases = []\n",
    "    for phrase in top_phrases:\n",
    "        print [(word,phen_matcher._f_span(word)) for word in phrase.text.split(' ')]\n",
    "        if any (phen_matcher._f_span(word) for word in phrase.text.split(' ')):\n",
    "            matching_phrases.append(phrase)\n",
    "            print phrase\n",
    "#         print phrase, [phen_matcher._f_span(word) for word in phrase.text.split(' ')]\n",
    "    print [LF(c) for LF in LF_tables]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = learner.predict_wmv(candidates)\n",
    "rels = [(c.span0.context.document.name, c.span0.get_span(), c.span1.get_span()) for (c, p) in zip(candidates, preds) if p == 1]\n",
    "print len(rels), 'relations extracted, e.g.:'\n",
    "print rels[:10]\n",
    "\n",
    "# store relations to annotate\n",
    "with open('rels.acronyms.extracted.tsv', 'w') as f:\n",
    "    for doc_id, str1, str2 in rels:\n",
    "        try:\n",
    "            out = u'{}\\t{}\\t{}\\n'.format(doc_id, unicode(str1), str2)\n",
    "            f.write(out.encode(\"UTF-8\"))\n",
    "        except:\n",
    "            print 'Error in saving:', str1, str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve acronyms based on ones extracted earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from extractor.dictionary import Dictionary, unravel\n",
    "\n",
    "D = Dictionary()\n",
    "D.load('acronyms.extracted.tsv')\n",
    "print len(D), 'definitions loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dictionary to resolve acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rels = [ (doc_id, rs_id, unravel(doc_id, phen, D)) for doc_id, rs_id, phen in rels ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate extracted relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate the recall w.r.t. GWAS Central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus.documents:\n",
    "    assocs = [assoc for assoc in kb.assoc_by_pmid(doc.name) if assoc.source == 'gwas_central' and assoc.pvalue < 1e-5]\n",
    "    print doc.name, len(assocs), len([(pmid, rsid, phen) for pmid, rsid, phen in new_rels if pmid == doc.name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ([(pmid, rsid, phen) for pmid, rsid, phen in new_rels if pmid == '17903305'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmids = sorted(list({pmid for pmid, _, _ in new_rels}))\n",
    "\n",
    "from db.kb import KnowledgeBase\n",
    "kb = KnowledgeBase()\n",
    "assocs = [assoc for pmid in pmids for assoc in kb.assoc_by_pmid(pmid) if assoc.source == 'gwas_central' and assoc.pvalue < 1e-5]\n",
    "print len(pmids), len(assocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect resolved relations\n",
    "rel_dict = { (pmid, rsid) : set() for (pmid, rsid, phen) in new_rels }\n",
    "for (pmid, rsid, phen) in new_rels:\n",
    "    rel_dict[(pmid, rsid)].add(phen)\n",
    "\n",
    "gold_rel_dict = { (a.paper.pubmed_id, a.snp.rs_id) : set() for a in assocs }\n",
    "for a in assocs:\n",
    "    gold_rel_dict[(a.paper.pubmed_id, a.snp.rs_id)].add(a.phenotype.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, evaluate recall: how many associations in GWAS central can we recover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in assocs[:500]:\n",
    "    s1 = gold_rel_dict[(a.paper.pubmed_id, a.snp.rs_id)]\n",
    "    s2 = rel_dict.get((str(a.paper.pubmed_id), a.snp.rs_id), {})\n",
    "    if len(s1) != 1 or len(s2) != 1:\n",
    "        print a.paper.pubmed_id, a.snp.rs_id, a.source\n",
    "        print 'GWC:', gold_rel_dict[(a.paper.pubmed_id, a.snp.rs_id)]\n",
    "        print 'US: ', rel_dict.get((str(a.paper.pubmed_id), a.snp.rs_id), None)\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second question: can we learn any more SNPs than the ones that are already in GWAS central?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmids = sorted(list({pmid for pmid, _, _ in new_rels if int(pmid) < 17903297}))\n",
    "\n",
    "from db.kb import KnowledgeBase\n",
    "kb = KnowledgeBase()\n",
    "assocs = [assoc for pmid in pmids for assoc in kb.assoc_by_pmid(pmid) if assoc.source == 'gwas_central']\n",
    "print len(assocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in assocs:\n",
    "    s1 = gold_rel_dict[(a.paper.pubmed_id, a.snp.rs_id)]\n",
    "    s2 = rel_dict.get((str(a.paper.pubmed_id), a.snp.rs_id), {})\n",
    "    print a.paper.pubmed_id, a.snp.rs_id, a.source\n",
    "    print 'GWC:', gold_rel_dict[(a.paper.pubmed_id, a.snp.rs_id)]\n",
    "    print 'US: ', rel_dict.get((str(a.paper.pubmed_id), a.snp.rs_id), None)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine with extracted pvalue/rsid relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval_rsid_dict = dict()\n",
    "pval_dict = dict() # combine all of the pvalues for a SNPs in the same document into one set\n",
    "with open('pval-rsid.raw.tsv') as f:\n",
    "    for line in f:\n",
    "        pmid, rsid, table_id, row_id, col_id, pval = line.strip().split('\\t')\n",
    "        pval, table_id, row_id, col_id = float(pval), int(table_id), int(row_id), int(col_id)\n",
    "        \n",
    "        if pmid not in pval_rsid_dict: pval_rsid_dict[pmid] = dict()\n",
    "        key = (rsid, table_id, row_id)\n",
    "        if key not in pval_rsid_dict[pmid]: pval_rsid_dict[pmid][key] = set()\n",
    "        pval_rsid_dict[pmid][key].add(pval)\n",
    "                \n",
    "        if pmid not in pval_dict: pval_dict[pmid] = dict()\n",
    "        if rsid not in pval_dict[pmid]: pval_dict[pmid][rsid] = set()\n",
    "        pval_dict[pmid][rsid].add(pval)\n",
    "\n",
    "pval_dict0 = {pmid : {rsid : min(pval_dict[pmid][rsid]) for rsid in pval_dict[pmid]} for pmid in pval_dict}\n",
    "pval_rsid_dict0 = {pmid : {key : min(pval_rsid_dict[pmid][key]) for key in pval_rsid_dict[pmid]} for pmid in pval_rsid_dict}\n",
    "pval_dict = pval_dict0\n",
    "pval_rsid_dict = pval_rsid_dict0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan. If phen/rsid has been extracted from tables: take its pvalue from pval_rsid_dict.\n",
    "\n",
    "If not, we assume that paper has only one phenotype and we take the smallest reported pvalue in the paper.\n",
    "\n",
    "Our goal for now is just to filter phen/rsid relations that have pval<1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all relations that are sufficiently small p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds = learner.predict_wmv(candidates)\n",
    "# predicted_candidates = [c for (c, p) in zip(candidates, preds) if p == 1]\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "def _normalize_str(s):\n",
    "    try:\n",
    "        s = s.encode('utf-8')\n",
    "        return s\n",
    "    except UnicodeEncodeError: \n",
    "        pass\n",
    "    try:\n",
    "        s = s.decode('utf-8')\n",
    "        return s\n",
    "    except UnicodeDecodeError: \n",
    "        pass    \n",
    "    raise Exception()\n",
    "\n",
    "with open('phen-rsid.table.rel.tsv', 'w') as f:\n",
    "    for c in predicted_candidates:\n",
    "        pmid = c.span0.context.document.name\n",
    "        rsid = c.span0.get_span()\n",
    "        phen = c.span1.get_span()        \n",
    "        table_id = c.span0.context.table.position\n",
    "        row_num = c.span0.context.cell.row_num\n",
    "        col_num = c.span0.context.cell.col_num # of the rsid\n",
    "\n",
    "        phen = (unravel(pmid, phen, D))\n",
    "        if isinstance(phen, unicode):\n",
    "            phen = phen.encode('utf-8')\n",
    "                    \n",
    "        pval = pval_rsid_dict[pmid].get((rsid, table_id, row_num), -1)\n",
    "        if pval > 1e-5: continue\n",
    "\n",
    "        out_str = '{pmid}\\t{rsid}\\t{phen}\\t{pval}\\ttable\\t{table_id}\\t{row}\\t{col}\\n'.format(\n",
    "                    pmid=pmid, rsid=rsid, phen=phen, pval=pval, table_id=table_id, row=row_num, col=col_num)\n",
    "        f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [(c, c.span0.context.cell.row_num, unravel(c.span0.context.document.name, c.span1.get_span(), D)) for c in candidates if c.span0.get_span() == 'rs10500631']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval_rsid_dict['17903294'].get(('rs10500631', 1, 5), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in pval_rsid_dict['17903294']:    \n",
    "    print x, pval_rsid_dict['17903294'][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
